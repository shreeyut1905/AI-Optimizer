{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NRN9oYpHJdfY"
      ],
      "authorship_tag": "ABX9TyPAejiusjgOXLZ3wVa0kBWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreeyut1905/AI-Optimizer/blob/main/AI_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "ZEY0-3LwZXJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U --quiet langchain langchain-chroma langchain-community openai langchain-experimental\n",
        "%pip install --quiet \"unstructured[all-docs]\" pypdf pillow pydantic lxml pillow matplotlib chromadb tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyWtMhS-iAZA",
        "outputId": "0eff90b1-3fe3-4757-bd64-9d927d8efef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.6/514.6 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6VQAM5oHSDk",
        "outputId": "8d72cb12-2f7c-493a-c007-017ecada5612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.61-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.48-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.2.2)\n",
            "Downloading langgraph-0.2.61-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m963.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.9-py3-none-any.whl (37 kB)\n",
            "Downloading langgraph_sdk-0.1.48-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langgraph-sdk, langgraph-checkpoint, langgraph\n",
            "Successfully installed langgraph-0.2.61 langgraph-checkpoint-2.0.9 langgraph-sdk-0.1.48\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_core langchain langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph"
      ],
      "metadata": {
        "id": "NRN9oYpHJdfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated , Any , Dict , Sequence , TypedDict\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "def merge_dicts(a:Dict[str,Any],b:Dict[str,Any]) -> Dict[str,Any]:\n",
        "  return {**a,**b}\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages:Annotated[Sequence[BaseMessage],operator.add]\n",
        "  date: Annotated[Dict[str,Any],merge_dicts]\n",
        "  metadata:Annotated[Dict[str,Any],merge_dicts]\n",
        "\n",
        "\n",
        "def show_agent_reasoning(output,agent_name):\n",
        "  print(f\"\\n{'=' * 10} {agent_name.center(28)} {'=' * 10}\")\n",
        "\n",
        "\n",
        "  def convert_to_serializable(obj):\n",
        "      if hasattr(obj, 'to_dict'):  # Handle Pandas Series/DataFrame\n",
        "          return obj.to_dict()\n",
        "      elif hasattr(obj, '__dict__'):  # Handle custom objects\n",
        "          return obj.__dict__\n",
        "      elif isinstance(obj, (int, float, bool, str)):\n",
        "          return obj\n",
        "      elif isinstance(obj, (list, tuple)):\n",
        "          return [convert_to_serializable(item) for item in obj]\n",
        "      elif isinstance(obj, dict):\n",
        "          return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
        "      else:\n",
        "          return str(obj)  # Fallback to string representation\n",
        "  if isintance(output,(dict,list)):\n",
        "    serializable_output = convert_to_serializable(output)\n",
        "    print(json.dumps(serializable_output,indent=2))\n",
        "  else:\n",
        "    try:\n",
        "      parsed_output = json.loads(output)\n",
        "      print(json.dumps(parsed_output,indent=2))\n",
        "    except json.JSONDecodeError:\n",
        "      print(output)\n",
        "  print(\"=\"*48)\n"
      ],
      "metadata": {
        "id": "BRPzUTneJgkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "AmdYjETyMuIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict, Any, List\n",
        "import pandas as pd\n",
        "import requests"
      ],
      "metadata": {
        "id": "V4osaM6wMx0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_financial_metrics(ticker:str,report_period:str,period:str = \"ttm\" , limit = 1) -> List[Dict[str,Any]]:\n",
        "  headers = {\"X-API-KEY\": os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")}\n",
        "  url = (\n",
        "        f\"https://api.financialdatasets.ai/financial-metrics/\"\n",
        "        f\"?ticker={ticker}\"\n",
        "        f\"&report_period_lte={report_period}\"\n",
        "        f\"&limit={limit}\"\n",
        "        f\"&period={period}\"\n",
        "    )\n",
        "  response = requests.get(url,headers = headers)\n",
        "  if response.status_code != 200 :\n",
        "    raise Exception(\n",
        "        f\"Error Fetching the data : {response.status_code} - {response.text}\"\n",
        "    )\n",
        "  data = response.json()\n",
        "  financial_metrics = data.get(\"financial_metrics\")\n",
        "  if financial_metrics:\n",
        "    raise ValueError(\"No Financial Metrics returned\")\n",
        "  return financial_metrics\n"
      ],
      "metadata": {
        "id": "sYc9yR9YMzUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utyVaZBmZ5qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_line_items(ticker:str,line_items:List[str],period:str = \"ttm\" ,limit : int =1 ) -> List[Dict[str,Any]]:\n",
        "  \"\"\"Fetch cash flow statements from the API.\"\"\"\n",
        "  headers = {\"X-API-KEY\": os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")}\n",
        "  url = \"https://api.financialdatasets.ai/financials/search/line-items\"\n",
        "  body = {\n",
        "      \"tickers\" : [ticker],\n",
        "      \"line_items\":line_items,\n",
        "      \"period\":period,\n",
        "      \"limit\":limit\n",
        "  }\n",
        "  response =  requests.post(url,headers = headers,json=body)\n",
        "  if response.status_code != 200:\n",
        "    raise Exception(\n",
        "        f\"Error Fetching data: {r5esponse.status_code} - {response.text}\"\n",
        "    )\n",
        "    data = response.json()\n",
        "    search_results = data.get(\"search_results\")\n",
        "    if not search_results:\n",
        "      raise ValueError(\"No search results returned\")\n",
        "    return search_results\n"
      ],
      "metadata": {
        "id": "C8lXwFB6QtEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_insider_trades(ticker:str,end_date:str,limit:int=5)-> List[Dict[str,Any]]:\n",
        "  \"\"\"\n",
        "    Fetch insider trades for a given ticker and date range.\n",
        "   \"\"\"\n",
        "  headers = {\"X-API-KEY\": os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")}\n",
        "  url = (\n",
        "      f\"https://api.financialdatasets.ai/insider-trades/\"\n",
        "      f\"?ticker={ticker}\"\n",
        "      f\"&filing_date_lte={end_date}\"\n",
        "      f\"&limit={limit}\"\n",
        "  )\n",
        "  response = requests.get(url,headers=headers)\n",
        "  if response.status_code != 200:\n",
        "    raise Exception(\n",
        "        f\"Error Fetching data: {response.status_code}  - {response.text}\"\n",
        "    )\n",
        "\n",
        "  data = response.json()\n",
        "  insider_trades = data.get(\"insider_trades\")\n",
        "  if not insider_trades:\n",
        "    raise ValueError(\"No Insider Trades returned\")\n",
        "  return insider_trades\n"
      ],
      "metadata": {
        "id": "gpjfidyiMxyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_market_cap(ticker:str)->List[Dict[str,Any]]:\n",
        "  \"\"\"Fetch market cap from the API.\"\"\"\n",
        "  headers = {\"X-API-KEY\": os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")}\n",
        "  url = (\n",
        "      f'https://api.financialdatasets.ai/company/facts'\n",
        "      f'?ticker={ticker}'\n",
        "  )\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if response.status_code != 200:\n",
        "      raise Exception(\n",
        "          f\"Error fetching data: {response.status_code} - {response.text}\"\n",
        "      )\n",
        "  data = response.json()\n",
        "  company_facts = data.get(\"company_facts\")\n",
        "  if not company_facts:\n",
        "      raise ValueError(\"No company facts returned\")\n",
        "  return company_facts.get('market_cap')\n",
        "\n"
      ],
      "metadata": {
        "id": "iypkQu-uMxvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prices(ticker:str,start_date:str,end_date:str)->List[Dict[str,Any]]:\n",
        "  \"\"\"Fetch prices data from the API.\"\"\"\n",
        "  headers = {\"X-API-KEY\": os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")}\n",
        "  url = (\n",
        "      f\"https://api.financialdatasets.ai/prices/\"\n",
        "      f\"?ticker={ticker}\"\n",
        "      f\"&interval=day\"\n",
        "      f\"&interval_multiplier=1\"\n",
        "      f\"&start_date={start_date}\"\n",
        "      f\"&end_date={end_date}\"\n",
        "  )\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if response.status_code != 200:\n",
        "      raise Exception(\n",
        "          f\"Error fetching data: {response.status_code} - {response.text}\"\n",
        "      )\n",
        "  data = response.json()\n",
        "  prices = data.get(\"prices\")\n",
        "  if not prices:\n",
        "      raise ValueError(\"No price data returned\")\n",
        "  return prices"
      ],
      "metadata": {
        "id": "0kqPcieJYGng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prices_to_df(prices: List[Dict[str, Any]]) -> pd.DataFrame:\n",
        "    \"\"\"Convert prices to a DataFrame.\"\"\"\n",
        "    df = pd.DataFrame(prices)\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"time\"])\n",
        "    df.set_index(\"Date\", inplace=True)\n",
        "    numeric_cols = [\"open\", \"close\", \"high\", \"low\", \"volume\"]\n",
        "    for col in numeric_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    df.sort_index(inplace=True)\n",
        "    return df\n",
        "def get_price_data(\n",
        "    ticker: str,\n",
        "    start_date: str,\n",
        "    end_date: str\n",
        ") -> pd.DataFrame:\n",
        "    prices = get_prices(ticker, start_date, end_date)\n",
        "    return prices_to_df(prices)"
      ],
      "metadata": {
        "id": "pRq_VIYDYGkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WZs-bq3ZNqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "EOXyhyeJH81e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fundamentals Agent"
      ],
      "metadata": {
        "id": "_8rgVMDeIAVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "import json\n",
        "def fundamentals_agent(state: AgentState):\n",
        "    \"\"\"Analyzes fundamental data and generates trading signals.\"\"\"\n",
        "    data = state[\"data\"]\n",
        "    end_date = data[\"end_date\"]\n",
        "\n",
        "\n",
        "    financial_metrics = get_financial_metrics(\n",
        "        ticker=data[\"ticker\"],\n",
        "        report_period=end_date,\n",
        "        period=\"ttm\",\n",
        "        limit=1,\n",
        "    )\n",
        "\n",
        "\n",
        "    metrics = financial_metrics[0]\n",
        "\n",
        "\n",
        "    signals = []\n",
        "    reasoning = {}\n",
        "\n",
        "\n",
        "    return_on_equity = metrics.get(\"return_on_equity\")\n",
        "    net_margin = metrics.get(\"net_margin\")\n",
        "    operating_margin = metrics.get(\"operating_margin\")\n",
        "\n",
        "    thresholds = [\n",
        "        (return_on_equity, 0.15),\n",
        "        (net_margin, 0.20),\n",
        "        (operating_margin, 0.15),\n",
        "    ]\n",
        "    profitability_score = sum(\n",
        "        metric is not None and metric > threshold for metric, threshold in thresholds\n",
        "    )\n",
        "\n",
        "    signals.append(\n",
        "        \"bullish\"\n",
        "        if profitability_score >= 2\n",
        "        else \"bearish\" if profitability_score == 0 else \"neutral\"\n",
        "    )\n",
        "    reasoning[\"profitability_signal\"] = {\n",
        "        \"signal\": signals[0],\n",
        "        \"details\": (\n",
        "            f\"ROE: {metrics['return_on_equity']:.2%}\"\n",
        "            if metrics[\"return_on_equity\"]\n",
        "            else \"ROE: N/A\"\n",
        "        )\n",
        "        + \", \"\n",
        "        + (\n",
        "            f\"Net Margin: {metrics['net_margin']:.2%}\"\n",
        "            if metrics[\"net_margin\"]\n",
        "            else \"Net Margin: N/A\"\n",
        "        )\n",
        "        + \", \"\n",
        "        + (\n",
        "            f\"Op Margin: {metrics['operating_margin']:.2%}\"\n",
        "            if metrics[\"operating_margin\"]\n",
        "            else \"Op Margin: N/A\"\n",
        "        ),\n",
        "    }\n",
        "\n",
        "\n",
        "    revenue_growth = metrics.get(\"revenue_growth\")\n",
        "    earnings_growth = metrics.get(\"earnings_growth\")\n",
        "    book_value_growth = metrics.get(\"book_value_growth\")\n",
        "\n",
        "    thresholds = [\n",
        "        (revenue_growth, 0.10),\n",
        "        (earnings_growth, 0.10),\n",
        "        (book_value_growth, 0.10),\n",
        "    ]\n",
        "    growth_score = sum(\n",
        "        metric is not None and metric > threshold for metric, threshold in thresholds\n",
        "    )\n",
        "\n",
        "    signals.append(\n",
        "        \"bullish\"\n",
        "        if growth_score >= 2\n",
        "        else \"bearish\" if growth_score == 0 else \"neutral\"\n",
        "    )\n",
        "    reasoning[\"growth_signal\"] = {\n",
        "        \"signal\": signals[1],\n",
        "        \"details\": (\n",
        "            f\"Revenue Growth: {metrics['revenue_growth']:.2%}\"\n",
        "            if metrics[\"revenue_growth\"]\n",
        "            else \"Revenue Growth: N/A\"\n",
        "        )\n",
        "        + \", \"\n",
        "        + (\n",
        "            f\"Earnings Growth: {metrics['earnings_growth']:.2%}\"\n",
        "            if metrics[\"earnings_growth\"]\n",
        "            else \"Earnings Growth: N/A\"\n",
        "        ),\n",
        "    }\n",
        "\n",
        "\n",
        "    current_ratio = metrics.get(\"current_ratio\")\n",
        "    debt_to_equity = metrics.get(\"debt_to_equity\")\n",
        "    free_cash_flow_per_share = metrics.get(\"free_cash_flow_per_share\")\n",
        "    earnings_per_share = metrics.get(\"earnings_per_share\")\n",
        "\n",
        "    health_score = 0\n",
        "    if current_ratio and current_ratio > 1.5:\n",
        "        health_score += 1\n",
        "    if debt_to_equity and debt_to_equity < 0.5:  #To know more about --> Conservative debt levels concept\n",
        "        health_score += 1\n",
        "    if (\n",
        "        free_cash_flow_per_share\n",
        "        and earnings_per_share\n",
        "        and free_cash_flow_per_share > earnings_per_share * 0.8\n",
        "    ):\n",
        "        health_score += 1\n",
        "\n",
        "    signals.append(\n",
        "        \"bullish\"\n",
        "        if health_score >= 2\n",
        "        else \"bearish\" if health_score == 0 else \"neutral\"\n",
        "    )\n",
        "    reasoning[\"financial_health_signal\"] = {\n",
        "        \"signal\": signals[2],\n",
        "        \"details\": (\n",
        "            f\"Current Ratio: {metrics['current_ratio']:.2f}\"\n",
        "            if metrics[\"current_ratio\"]\n",
        "            else \"Current Ratio: N/A\"\n",
        "        )\n",
        "        + \", \"\n",
        "        + (\n",
        "            f\"D/E: {metrics['debt_to_equity']:.2f}\"\n",
        "            if metrics[\"debt_to_equity\"]\n",
        "            else \"D/E: N/A\"\n",
        "        ),\n",
        "    }\n",
        "\n",
        "\n",
        "    pe_ratio = metrics.get(\"price_to_earnings_ratio\")\n",
        "    pb_ratio = metrics.get(\"price_to_book_ratio\")\n",
        "    ps_ratio = metrics.get(\"price_to_sales_ratio\")\n",
        "\n",
        "    thresholds = [\n",
        "        (pe_ratio, 25),\n",
        "        (pb_ratio, 3),\n",
        "        (ps_ratio, 5),\n",
        "    ]\n",
        "    price_ratio_score = sum(\n",
        "        metric is not None and metric > threshold for metric, threshold in thresholds\n",
        "    )\n",
        "\n",
        "    signals.append(\n",
        "        \"bullish\"\n",
        "        if price_ratio_score >= 2\n",
        "        else \"bearish\" if price_ratio_score == 0 else \"neutral\"\n",
        "    )\n",
        "    reasoning[\"price_ratios_signal\"] = {\n",
        "        \"signal\": signals[3],\n",
        "        \"details\": (f\"P/E: {pe_ratio:.2f}\" if pe_ratio else \"P/E: N/A\")\n",
        "        + \", \"\n",
        "        + (f\"P/B: {pb_ratio:.2f}\" if pb_ratio else \"P/B: N/A\")\n",
        "        + \", \"\n",
        "        + (f\"P/S: {ps_ratio:.2f}\" if ps_ratio else \"P/S: N/A\"),\n",
        "    }\n",
        "\n",
        "\n",
        "    bullish_signals = signals.count(\"bullish\")\n",
        "    bearish_signals = signals.count(\"bearish\")\n",
        "\n",
        "    if bullish_signals > bearish_signals:\n",
        "        overall_signal = \"bullish\"\n",
        "    elif bearish_signals > bullish_signals:\n",
        "        overall_signal = \"bearish\"\n",
        "    else:\n",
        "        overall_signal = \"neutral\"\n",
        "\n",
        "\n",
        "    total_signals = len(signals)\n",
        "    confidence = round(max(bullish_signals, bearish_signals) / total_signals, 2) * 100\n",
        "\n",
        "    message_content = {\n",
        "        \"signal\": overall_signal,\n",
        "        \"confidence\": confidence,\n",
        "        \"reasoning\": reasoning,\n",
        "    }\n",
        "\n",
        "\n",
        "    message = HumanMessage(\n",
        "        content=json.dumps(message_content),\n",
        "        name=\"fundamentals_agent\",\n",
        "    )\n",
        "\n",
        "\n",
        "    if state[\"metadata\"][\"show_reasoning\"]:\n",
        "        show_agent_reasoning(message_content, \"Fundamental Analysis Agent\")\n",
        "\n",
        "\n",
        "    state[\"data\"][\"analyst_signals\"][\"fundamentals_agent\"] = {\n",
        "        \"signal\": overall_signal,\n",
        "        \"confidence\": confidence,\n",
        "        \"reasoning\": reasoning,\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"messages\": [message],\n",
        "        \"data\": data,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "FOdNiTEBH8mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.prompts import  ChatPromptTemplate\n",
        "from langchain_community.chat_models import ChatVertexAI\n",
        "def portfolio_management_agent(state:AgentState):\n",
        "  template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are a portfolio manager making final trading decisions.\n",
        "            Your job is to make a trading decision based on the team's analysis.\n",
        "\n",
        "            Provide the following in your output:\n",
        "            - \"action\": \"buy\" | \"sell\" | \"hold\",\n",
        "            - \"quantity\": <positive integer>\n",
        "            - \"confidence\": <float between 0 and 1>\n",
        "            - \"reasoning\": <concise explanation of the decision including how you weighted the signals>\n",
        "\n",
        "            Trading Rules:\n",
        "            - Only buy if you have available cash\n",
        "            - Only sell if you have shares to sell\n",
        "            - Quantity must be ≤ current position for sells\n",
        "            - Quantity must be ≤ max_position_size from risk management\"\"\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"\"\"Based on the team's analysis below, make your trading decision.\n",
        "\n",
        "            Technical Analysis Trading Signal: {technical_signal}\n",
        "            Fundamental Analysis Trading Signal: {fundamentals_signal}\n",
        "            Sentiment Analysis Trading Signal: {sentiment_signal}\n",
        "            Valuation Analysis Trading Signal: {valuation_signal}\n",
        "            Risk Management Position Limit: {max_position_size}\n",
        "            Here is the current portfolio:\n",
        "            Portfolio:\n",
        "            Cash: {portfolio_cash}\n",
        "            Current Position: {portfolio_stock} shares\n",
        "\n",
        "            Only include the action, quantity, reasoning, and confidence in your output as JSON.  Do not include any JSON markdown.\n",
        "\n",
        "            Remember, the action must be either buy, sell, or hold.\n",
        "            You can only buy if you have available cash.\n",
        "            You can only sell if you have shares in the portfolio to sell.\n",
        "            \"\"\",\n",
        "        ),\n",
        "    ]\n",
        "  )\n",
        "  portfolio = state[\"data\"][\"portfolio\"]\n",
        "  analyst_signals = state[\"data\"][\"analyst_signals\"]\n",
        "\n",
        "  prompt = template.invoke(\n",
        "      {\n",
        "        \"technical_signal\":analyst_signals[\"technical_analyst_agent\"][\"signal\"],\n",
        "        \"funadmentals_signal\":analyst_signals[\"fundamentals_agent\"][\"signal\"],\n",
        "        \"sentiment_signal\":analyst_signal[\"sentiment_agent\"]\n",
        "\n",
        "      }\n",
        "  )\n"
      ],
      "metadata": {
        "id": "oMmGov5wIuGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qcIVeLYPpnzD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}